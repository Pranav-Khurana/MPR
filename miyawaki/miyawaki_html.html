<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <title>Miyawaki</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="icon" href="../images/favicon.png" type="image/x-icon">
    <!-- Google Fonts -->
    <link
        href="https://fonts.googleapis.com/css?family=DM+Sans:100,200,300,400,600,500,700,800,900|DM+Sans:100,200,300,400,500,600,700,800,900&amp;subset=latin"
        rel="stylesheet">
    <!-- Bootstrap 4.3.1 CSS -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css"
        integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
    <!-- Slick 1.8.1 jQuery plugin CSS (Sliders) -->
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/slick-carousel@1.8.1/slick/slick.css" />
    <!-- Fancybox 3 jQuery plugin CSS (Open images and video in popup) -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css" />
    <!-- AOS 2.3.4 jQuery plugin CSS (Animations) -->
    <link href="https://unpkg.com/aos@2.3.4/dist/aos.css" rel="stylesheet">
    <!-- FontAwesome CSS -->
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.1/css/all.css"
        integrity="sha384-50oBUHEmvpQ+1lW4y57PTFmhCaXp0ML5d60M1M7uH2+nqUivzIebhndOJK28anvf" crossorigin="anonymous">
    <!-- Startup 3 CSS (Styles for all blocks) -->
    <link href="../css/style.css" rel="stylesheet" />
    <!-- jQuery 3.3.1 -->
    <script src="https://code.jquery.com/jquery-3.3.1.min.js"></script>
</head>

<body>
    <!-- Navigation Mobile type 1 -->

    <a href="#" class="open_menu bg-light radius_full"><i class="fas fa-bars lh-40"></i></a>
    <div class="navigation_mobile bg-dark type1">
        <a href="#" class="close_menu color-white"><i class="fas fa-times"></i></a>
        <div class="px-40 pt-60 pb-60 inner">
            <div><a href="../index.html" class="f-heading f-22 link color-white mb-20">Home</a></div>
            <div><a href="../haxby/haxby_html.html" class="f-heading f-22 link color-white mb-20">Haxby</a></div>
            <div><a href="../localizer/localizer_html.html" class="f-heading f-22 link color-white mb-20">Analysis of
                    Motor Task</a></div>
            <div><a href="../visualization/visualization_html.html"
                    class="f-heading f-22 link color-white mb-20">Visualizing Nifti Files</a></div>
        </div>
    </div>

    <header>
        <nav class="header_menu_1 pt-10 pb-10 mt-10">
            <div class="container px-xl-0">
                <div class="row justify-content-center align-items-center f-18 medium">
                    <div class="col-lg-8 text-center" data-aos-duration="600" data-aos="fade-down" data-aos-delay="300">
                        <a href="../index.html" class="link color-heading mx-15">Home</a>
                        <a href="../haxby/haxby_html.html" class="link color-heading mx-15">Haxby</a>
                        <a href="../localizer/localizer_html.html" class="link color-heading mx-15">Analysis of Motor
                            Task</a>
                        <a href="../visualization/visualization_html.html" class="link color-heading mx-15">Visualizing
                            Nifti Files</a>
                        
                    </div>
                </div>
            </div>
        </nav>

        <div class="container">
            <br>
            <br><br>
            <h2 class="big text-center" data-aos-duration="600" data-aos="fade-down" data-aos-delay="0">Miyawaki
                Experiment</h2>
            <div class="big mw-1000 mx-auto mt-30 f-22 color text-center text-adaptive" data-aos-duration="600"
                data-aos="fade-down" data-aos-delay="300">Perceptual experience consists of an enormous number of
                possible states. Previous fMRI studies
                have predicted a perceptual state by classifying brain
                activity into prespecified categories. Constraint-free
                visual image reconstruction is more challenging, as
                it is impractical to specify brain activity for all possible
                images.
            </div>
            <div class="big mw-1000 mx-auto mt-30 f-22 color text-center text-adaptive" data-aos-duration="600"
                data-aos="fade-down" data-aos-delay="300">
                <img src="file2.png" alt="Miyawaki experiment">
            </div>
            <div class="big mw-1000 mx-auto mt-30 f-22 color text-center text-adaptive" data-aos-duration="600"
                data-aos="fade-down" data-aos-delay="300">
                In this study, we reconstructed visual images
                by combining local image bases of multiple scales,
                whose contrasts were independently decoded from
                fMRI activity by automatically selecting relevant voxels and exploiting their correlated patterns.
                Binary
                contrast, 10*10 patch images (2<sup>100 </sup> possible states)
                were accurately reconstructed without any image
                prior on a single trial or volume basis by measuring
                brain activity only for several hundred random
                images. Reconstruction was also used to identify
                the presented image among millions of candidates.
                The results suggest that our approach provides an effective means to read out complex perceptual states
                from brain activity while discovering information
                representation in multivoxel patterns.
            </div>
            <div class="big mw-1000 mx-auto mt-30 f-22 color text-center text-adaptive" data-aos-duration="600"
                data-aos="fade-down" data-aos-delay="300">
                <img src="file1.png" alt="Miyawaki experiment">
            </div>
            <div class="big mw-1000 mx-auto mt-30 f-22 color text-center text-adaptive" data-aos-duration="600"
                data-aos="fade-down" data-aos-delay="300">
                In the present study, we attempted to reconstruct visual images
                defined by binary contrast patterns consisting of 10*10 square
                patches. We used local image bases of four scales: 1*1, 1*2, 2*1,
                and 2*2 patch areas. They were placed at every location in the image with overlaps. Although image
                elements larger than 2*2 or those
                with nonrectangular shapes could be used, the addition of such
                elements did not improve the reconstruction performance. fMRI signals were measured while subjects
                viewed a sequence
                of visual images consisting of binary contrast patches on a 10*10
                grid. In the ‘‘random image session,’’ a random pattern was
                presented for 6s followed by a 6s rest period. A total
                of 440 different random images were shown (each presented
                once).
            </div><br>
        </div>
    </header>


    <!-- Team 2 -->

    <section class="text-left ">
        
        <div class="container px-xl-0">
            <div class="row justify-content-left color-white">
                <div class="col-xl-12 col-lg-10">
                    <div class="mt-15 f-22 op-7 text-adaptive">
                        <div data-aos-duration="600" data-aos="fade-down" data-aos-delay="0">
                            <section>
                                <div align="center">
                                    <button class="button" onclick='func()'>View Code Here</button>
                                    <button class="button"><a
                                            href="https://colab.research.google.com/drive/1sNZPrzeyUkhytWUDPsCPFoAB6asUuy69" target="blank">Run
                                            Code Here</a> 
                                    </button>
                                            <br>
                                            <br>
                                </div>
                                <div id="code" style="background-color:black; display: none;">
                                    <div class="highlight">
                                        <code>
<pre>
	<font color="white">
        #Importing required libraries and dataset
        import numpy as np
        from nilearn.input_data import MultiNiftiMasker
        from sklearn.linear_model import OrthogonalMatchingPursuit as OMP
        from sklearn.feature_selection import f_classif, SelectKBest
        from sklearn.pipeline import Pipeline
        from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score)
        from matplotlib import pyplot as plt
        from nilearn.plotting import show
        from nilearn import datasets
        
        #Fetching the Dataset
        miyawaki_dataset = datasets.fetch_miyawaki2008()
        
        X_random = miyawaki_dataset.func[12:]
        X_figure = miyawaki_dataset.func[:12]
        y_random = miyawaki_dataset.label[12:]
        y_figure = miyawaki_dataset.label[:12]
        y_shape = (10, 10)
        
        #Load and mask fMRI data
        masker = MultiNiftiMasker(mask_img=miyawaki_dataset.mask, detrend=True, standardize=False)
        masker.fit()
        X_train = masker.transform(X_random)
        X_test = masker.transform(X_figure)
        
        #We load the visual stimuli from csv files
        y_train = []
        for y in y_random:
            y_train.append(np.reshape(np.loadtxt(y, dtype=np.int, delimiter=','), (-1,) + y_shape, order='F'))
        
        y_test = []
        for y in y_figure:
            y_test.append(np.reshape(np.loadtxt(y, dtype=np.int, delimiter=','), (-1,) + y_shape, order='F'))
        
        X_train = np.vstack([x[2:] for x in X_train])
        y_train = np.vstack([y[:-2] for y in y_train]).astype(float)
        X_test = np.vstack([x[2:] for x in X_test])
        y_test = np.vstack([y[:-2] for y in y_test]).astype(float)
        
        n_pixels = y_train.shape[1]
        n_features = X_train.shape[1]
        
        
        def flatten(list_of_2d_array):
            flattened = []
            for array in list_of_2d_array:
                flattened.append(array.ravel())
            return flattened
        
        
        #Build the design matrix for multiscale computation
        #Matrix is squared, y_rows == y_cols
        y_cols = y_shape[1]
        
        #Original data
        design_matrix = np.eye(100)
        
        
        #Example of matrix used for multiscale (sum pixels vertically)
        #
        # 0.5 *
        #
        # 1 1 0 0 0 0 0 0 0 0
        # 0 1 1 0 0 0 0 0 0 0
        # 0 0 1 1 0 0 0 0 0 0
        # 0 0 0 1 1 0 0 0 0 0
        # 0 0 0 0 1 1 0 0 0 0
        # 0 0 0 0 0 1 1 0 0 0
        # 0 0 0 0 0 0 1 1 0 0
        # 0 0 0 0 0 0 0 1 1 0
        # 0 0 0 0 0 0 0 0 1 1
        
        height_tf = (np.eye(y_cols) + np.eye(y_cols, k=1))[:y_cols - 1] * .5
        width_tf = height_tf.T
        
        yt_tall = [np.dot(height_tf, m) for m in y_train]
        yt_large = [np.dot(m, width_tf) for m in y_train]
        yt_big = [np.dot(height_tf, np.dot(m, width_tf)) for m in y_train]
        
        #Add it to the training set
        y_train = [np.r_[y.ravel(), t.ravel(), l.ravel(), b.ravel()] for y, t, l, b in zip(y_train, yt_tall, yt_large, yt_big)]
        
        y_test = np.asarray(flatten(y_test))
        y_train = np.asarray(y_train)
        
        #Remove rest period
        X_train = X_train[y_train[:, 0] != -1]
        y_train = y_train[y_train[:, 0] != -1]
        X_test = X_test[y_test[:, 0] != -1]
        y_test = y_test[y_test[:, 0] != -1]
        
        #We define our Prediction function
        
        #Create as many OMP(OrthogonalMatchingPursuit) as voxels to predict
        clfs = []
        n_clfs = y_train.shape[1]
        for i in range(y_train.shape[1]):
            clf = Pipeline([('selection', SelectKBest(f_classif, 500)),  ('clf', OMP(n_nonzero_coefs=10))])
            clf.fit(X_train, y_train[:, i])
            clfs.append(clf)
        
        #Run the prediction function
        
        y_pred = []
        for clf in clfs:
            y_pred.append(clf.predict(X_test))
        y_pred = np.asarray(y_pred).T
        
        
        # We need to the multi scale reconstruction
        def split_multi_scale(y, y_shape):
            #Split data into 4 original multi_scale images
            yw, yh = y_shape
        
            #Index of original image
            split_index = [yw * yh]
            #Index of large image
            split_index.append(split_index[-1] + (yw - 1) * yh)
            #Index of tall image
            split_index.append(split_index[-1] + yw * (yh - 1))
            #Index of big image
            split_index.append(split_index[-1] + (yw - 1) * (yh - 1))
        
            #We split according to computed indices
            y_preds = np.split(y, split_index, axis=1)
        
            #y_pred is the original image
            y_pred = y_preds[0]
        
            #y_pred_tall is the image with 1x2 patch application. We have to make
            #some calculus to get it back in original shape
            height_tf_i = (np.eye(y_cols) + np.eye(y_cols, k=-1))[:, :y_cols - 1] * .5
            height_tf_i.flat[0] = 1
            height_tf_i.flat[-1] = 1
            y_pred_tall = [np.dot(height_tf_i, np.reshape(m, (yw - 1, yh))).flatten()
                           for m in y_preds[1]]
            y_pred_tall = np.asarray(y_pred_tall)
        
            #y_pred_large is the image with 2x1 patch application. We have to make
            #some calculus to get it back in original shape
            width_tf_i = (np.eye(y_cols) + np.eye(y_cols, k=1))[:y_cols - 1] * .5
            width_tf_i.flat[0] = 1
            width_tf_i.flat[-1] = 1
            y_pred_large = [np.dot(np.reshape(m, (yw, yh - 1)), width_tf_i).flatten()
                            for m in y_preds[2]]
            y_pred_large = np.asarray(y_pred_large)
        
            #y_pred_big is the image with 2x2 patch application. We use previous
            #matrices to get it back in original shape
            y_pred_big = [np.dot(np.reshape(m, (yw - 1, yh - 1)), width_tf_i)
                          for m in y_preds[3]]
            y_pred_big = [np.dot(height_tf_i, np.reshape(m, (yw - 1, yh))).flatten()
                          for m in y_pred_big]
            y_pred_big = np.asarray(y_pred_big)
        
            return (y_pred, y_pred_tall, y_pred_large, y_pred_big)
        
        
        y_pred, y_pred_tall, y_pred_large, y_pred_big = split_multi_scale(y_pred, y_shape)
        
        y_pred = (.25 * y_pred + .25 * y_pred_tall + .25 * y_pred_large + .25 * y_pred_big)
        
        #Check the Scores of the model
        print("Scores")
        print("------")
        print("  - Accuracy (percent): %f" % np.mean([
            accuracy_score(y_test[:, i], y_pred[:, i] > .5) for i in range(100)]))
        print("  - Precision: %f" % np.mean([
            precision_score(y_test[:, i], y_pred[:, i] > .5) for i in range(100)]))
        print("  - Recall: %f" % np.mean([
            recall_score(y_test[:, i], y_pred[:, i] > .5) for i in range(100)]))
        print("  - F1-score: %f" % np.mean([
            f1_score(y_test[:, i], y_pred[:, i] > .5) for i in range(100)]))
            
        #Finally we plot the Images
        for i in range(6):
            j = 10 * i
            fig = plt.figure()
            sp1 = plt.subplot(131)
            sp1.axis('off')
            plt.title('Stimulus')
            sp2 = plt.subplot(132)
            sp2.axis('off')
            plt.title('Reconstruction')
            sp3 = plt.subplot(133)
            sp3.axis('off')
            plt.title('Binarized')
            sp1.imshow(np.reshape(y_test[j], (10, 10)), cmap=plt.cm.gray,
                       interpolation='nearest'),
            sp2.imshow(np.reshape(y_pred[j], (10, 10)), cmap=plt.cm.gray,
                       interpolation='nearest'),
            sp3.imshow(np.reshape(y_pred[j] > .5, (10, 10)), cmap=plt.cm.gray,
                       interpolation='nearest')
          
        show()
	</font>
</pre>
</code>
                                    </div>
                                </div>
                                <div align="center">
                                    <table style="width:100%">
                                        <tr>
                                            <th>
                                                <p class="col-xl-12"><img width=100% height=100% id="image1" alt="OUTPUT"></p>
                                            </th>
                                            <th>
                                                <p class="col-xl-12"><img width=100% height=100% id="image2" alt="OUTPUT"></p>
                                            </th>
                                        </tr>
                                        <tr>
                                            <th>
                                                <p class="col-xl-12"><img width=100% height=100% id="image3" alt="OUTPUT"></p>
                                            </th>
                                            <th>
                                                <p class="col-xl-12"><img width=100% height=100% id="image4" alt="OUTPUT"></p>
                                            </th>
                                        </tr>
                                        <tr>
                                            <th>
                                                <p class="col-xl-12"><img width=100% height=100% id="image5" alt="OUTPUT"></p>
                                            </th>
                                            <th>
                                                <p class="col-xl-12"><img width=100% height=100% id="image6" alt="OUTPUT"></p>
                                            </th>
                                        </tr>
                                    </table>
                                </div>
                            </section>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <script>
        function func() {
            document.getElementById("image1").src = "miyawaki2008_reconstruction_0.png";
            document.getElementById("image2").src = "miyawaki2008_reconstruction_1.png";
            document.getElementById("image3").src = "miyawaki2008_reconstruction_2.png";
            document.getElementById("image4").src = "miyawaki2008_reconstruction_3.png";
            document.getElementById("image5").src = "miyawaki2008_reconstruction_4.png";
            document.getElementById("image6").src = "miyawaki2008_reconstruction_5.png";
            var div = document.getElementById("code");
            div.style.display = "block";
        }
    </script>

    <!-- Bootstrap 4.3.1 JS -->
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.bundle.min.js"></script>
    <!-- Fancybox 3 jQuery plugin JS (Open images and video in popup) -->
    <script src="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>
    <!-- Google maps JS API -->
    <script type="text/javascript"
        src="https://maps.googleapis.com/maps/api/js?v=3&key=AIzaSyDP6Ex5S03nvKZJZSvGXsEAi3X_tFkua4U"></script>
    <!-- Slick 1.8.1 jQuery plugin JS (Sliders) -->
    <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/slick-carousel@1.8.1/slick/slick.min.js"></script>
    <!-- AOS 2.3.4 jQuery plugin JS (Animations) -->
    <script src="https://unpkg.com/aos@2.3.4/dist/aos.js"></script>
    <!-- Maskedinput jQuery plugin JS (Masks for input fields) -->
    <script src="../js/jquery.maskedinput.min.js"></script>
    <!-- Startup 3 JS (Custom js for all blocks) -->
    <script src="../js/script.js"></script>

</body>

</html>